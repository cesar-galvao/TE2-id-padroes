---
title: "Lista 6"
author: 
  - César A. Galvão - 190011572
  - Gabriela Carneiro - 180120816
  - João Vitor Vasconcelos - 170126064
  - Kevyn Andrade de Souza - 190015853
lang: pt
execute:
  message: false
  warning: false
format: 
  pdf:
    geometry:
      - top=30mm
      - left=30mm
      - right=30mm
      - heightrounded
    code-overflow: wrap
    df-print: paged
    documentclass: article
    fig-pos: H
    tbl-pos: H
    cite-method: citeproc
    papersize: a4paper
    keep-tex: true
    mathspec: true
    toc: true
    toc-depth: 2
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \usepackage{bbm}
         \usepackage[auth-lg]{authblk}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
         \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
bibliography: references.bib
---

{{< pagebreak >}}

```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false

if (!("pacman" %in% installed.packages())){
  install.packages("pacman")
}

pacman::p_load(tidyverse, cowplot, latex2exp, ks, class, PerformanceAnalytics, MASS, magrittr)
```

# Questão 14

Estudar o pacote rsample em https://rsample.tidymodels.org/ e apresentar um exemplo
utilizando validação cruzada e Bootstrap.

------------------------------------------------------------------------

\ 


\ 

# Questão 15

Selecionar ou gerar um conjunto de dados e comparar a classificação após estimação de densidades utilizando os seguintes métodos:

1. Método do Histograma
2. Estimação baseada em Núcleos
3. k-Vizinhos mais Próximos

------------------------------------------------------------------------

\ 

Como nas notas de aula o exemplo usado para a classificação foi o conjunto de dados `faithful`, vamos usar o conjunto `iris` para comparar a classificação após estimação de densidades utilizando os métodos citados.

## Método do Histograma

Com o método do histograma, pretende-se agregar $\mathbf{X} = \{ X_1, \dots, X_n\}, \, X_i \overset{iid}{\sim} f(x)$ em intervalos da forma $[ x_0, x_0+h)$ e usar a frequência relativa de $\{x_n\}$ para aproximar a densidade $f(x)$. A estimativa é feita por

\begin{align}
f(x_0) = F'(x_0) = \lim_{h\to 0^+} \frac{F(x_0 + h) - F(x_0)}{h} = \lim_{h\to 0^+} \frac{P[x_0 < X < x_0 + h]}{h} \label{eq:histograma},
\end{align}

\ 

\noindent estabelecida a partir de uma origem $t_0$ e um comprimento (*binwidth*) $h > 0$. O histograma é então construído contando o número de pontos em intervalos chamados de *bins*, definidos por $\{ I_k : \, [t_k, t_{k+1}); \, t_k = t_0 + hk, \, k \in \mathcal{Z} \}$. O histograma de densidade no ponto $x$ é definido como

$$
\hat{f}_H(x; t_0, h) = \frac{1}{nh} \sum_{i=1}^n \mathbf{1}_{ \{x_i \in I_k \} }.
$$
\ 

Fica evidente a dependência em $t_0$, que pode ser evitada com um estimador *naïve* dado por

$$
f(x) = F'(x) = \lim_{h\to 0^+} \frac{F(x+h) - F(x-h)}{2h} = \lim_{h\to 0^+} \frac{P[x-h < X < x+h]}{2h}.
$$
\ 

\noindent Dessa forma, o histograma no ponto $x$ é definido como 

$$
\hat{f}_N(x;h) = \frac{1}{2nh} \sum_{i=1}^n \mathbf{1}_{ \{ x-h < X_i < x+h \} }.
$$

\ 

Para comparação, vamos usar o pacote `ggplot2` para gerar o histograma da variável `Petal.Length` do conjunto de dados `iris`, considerando $h = 0,2$.
```{r}
#| label: data-iris
#| include: false
data(iris)
```

```{r}
#| label: fig-histogramairis
#| fig-cap: Histograma de Petal Length do conjunto de dados iris.
#| fig-height: 2.75
#| echo: false

ggplot(iris, aes(x = Petal.Length, fill = Species)) +
  geom_histogram(binwidth = 0.2, alpha = 0.5, position = "identity") +
  theme_bw() +
  labs(x = "Petal.Length",
    y = "Frequência",
    fill = "Espécie") +
  scale_x_continuous(breaks = seq(0, 7, by = 1))+
  xlim(0, NA)+
  theme(panel.grid.minor.x = element_line())

```

\ 

Os histogramas na @fig-histogramairis2 foram criados manualmente com o mesmo binwidth, porém com origens variando entre $t_0 = 0$ e $t_0 = 1$ para ilustrar a limitação da dependência deste termo. Depreende-se portanto que está sendo usada a equação \ref{eq:histograma}.

\ 

```{r}
#| label: fig-histogramairis2
#| fig-cap: Histograma manual de Petal Length do conjunto de dados iris utilizando h = 0,2.

# bindiwdth
bk1 <- seq(0.5,7.1,by = 0.2)
bk2 <- seq(1, 7, by = 0.2)

par(mfrow = c(1,2))

hist(iris$Petal.Length, probability = T, breaks = bk1,
     ylim = c(0,0.8), col = 'blue',
     xlab = 't0 = 0.5, h = 0.2', main = "")
rug(iris$Petal.Length)

hist(iris$Petal.Length, probability = T, breaks = bk2,
     ylim = c(0,0.8), col = 'blue',
     xlab = 't0 = 1, h = 0.2', main = "")
rug(iris$Petal.Length)
```

\ 

Para a classificação, será determinada uma fronteira entre as espécies. Além disso, serão reservados pontos para treino e alguns para teste, que serão usados nos demais itens desta questão.

\ 

```{r}
#| label: data-iris2traintest

# iris com as variáveis selecionadas
iris2 <- iris %>% dplyr::select(Petal.Length, Petal.Width, Species)

# separação arbitrária de treino e teste
iris2 <- iris2 %>%
  mutate(partition = case_when(
    Petal.Length == 5.1 & Petal.Width %in% c(1.5, 1.6) ~ "teste",
    Petal.Length == 5 & Petal.Width %in% c(1.7, 1.5) ~ "teste",
    Petal.Length == 1.9 & Petal.Width == 0.4 ~ "teste",
    TRUE ~ "treino"
  ))
```

\ 

Os histogramas para cada espécie são estimados a seguir:

\ 

```{r}
#| label: histogramas-especies
#| output: false

#setosa
hist_setosa <- iris2 %>%
  dplyr::filter(Species == "setosa", partition == "treino") %$%
  hist(Petal.Length, probability = T, breaks = bk1)

#virginica
hist_virginica <- iris2 %>%
  dplyr::filter(Species == "virginica", partition == "treino") %$%
  hist(Petal.Length, probability = T, breaks = bk1)

#versicolor
hist_versicolor <- iris2 %>%
  dplyr::filter(Species == "versicolor", partition == "treino") %$%
  hist(Petal.Length, probability = T, breaks = bk1)

```

\ 

Finalmente, são acessados os resultados dos histogramas para construir a @tbl-histogramasespecies a seguir, em que apenas as *bins* com contagens superiores a zero são exibidas. As colunas da tabela correspondem ao valor inicial de cada *bin*, a contagem de observações, a densidade e a espécie.

Pode-se observar que a espécie Setosa está em uma região de comprimento de pétala bem separada das demais, então, para a classificação, basta verificar se uma nova observação se encontra na mesma região que as demais. Para as espécies Virginica e Versicolor, a separação não é tão clara, então é necessário adotar uma regra. Como a densidade do *bin* com início em 4.5 é maior para Versicolor e o inverso ocorre para o *bin* com início em 4.7, a fronteira entre esses bins será a regra de decisão. Isso implica em prováveis 2% de erro de classificação das Virgínica, quando se classifica em Versicolor, e 8.4% de erro de classificação das Versicolor, quando se classifica em Virginica[^1].

[^1]: Erros calculados a partir da sobreposição dos histogramas, considerando a fronteira de decisão.


\ 

```{r}
#| label: tbl-histogramasespecies
#| tbl-cap: Histogramas de Petal Length do conjunto de dados iris.

options(knitr.kable.NA = '-')

#monta a tabela com dados dos histogramas
tibble(
  ti = hist_setosa$breaks[-34],
  cont.setosa = hist_setosa$counts,
  dens.setosa = hist_setosa$density,
  cont.virginica = hist_virginica$counts,
  dens.virginica = hist_virginica$density,
  cont.versicolor = hist_versicolor$counts,
  dens.versicolor = hist_versicolor$density
) %>%
  # seleciona linhas com valores maiores que zero
  dplyr::filter(if_any(-ti, ~ . > 0)) %>%
  # ajusta decimais para arrumar o tamanho da tabela
  mutate(across(everything(), ~ round(., 2)),
         across(everything(), ~ if_else(. == 0, NA, .))) %>% 
  # gera tabela tex
  knitr::kable(
    align = "c"
  )

```

\ 

Em posse das regras de decisão, classifica-se os pontos de teste e se obtém a @tbl-histogramasespecies2 a seguir.

\ 

```{r}
#| label: tbl-histogramasespecies2
#| tbl-cap: Classificação dos pontos de teste do conjunto de dados iris via histograma.

iris2 %>%
  dplyr::filter(partition == "teste") %>%
  dplyr::select(-Petal.Width, -partition) %>%
  mutate(`Classificação` = c("setosa", rep("virginica", 4))) %>%
  knitr::kable(align = "cll")
  
```


\ 
 
## Estimação baseada em Núcleos

Soliciona outro problema do Método do Histograma, que é a necessidade de se utilizar intervalos pequenos e $n$ grande para se obter uma boa aproximação da densidade alvo. O estimador de densidade no caso univariado é dado por 


$$
\hat{f}(x; h) = \frac{1}{n} \sum_{i=1}^n k \,h(x - X_i),
$$
\ 

enquanto no caso multivariado é dado por

$$
\hat{f}(x; h) = \frac{1}{n |\mathbf{H}|^{1/2}} \sum_{i=1}^n k(\mathbf{H}^{-1/2}(\mathbf{x} - \mathbf{X}_i)).
$$
\ 

Nas expressões $k$ é o núcleo de uma função densidade arbitrária. Um problema que surge é a seleção do bandwidth $h$, que será demonstrado a seguir utilizando o pacote `ks` para a obtenção da matriz $\mathbf{H}_{p\times p}$ de intervalos[^2]. 

[^2]: Para o caso unidimensional, i.e. $p = 1$, $\mathbf{H} = h^2$.

A seguir é estimada a densidade conjunta para as variáveis `Petal.Length` e `Petal.Width` com o conjunto de teste, seguida da @fig-kdeiris ilustrando-a. Os pontos de cor azul correspondem aos pontos de teste.

\ 


```{r}
#| label: kdeiris

# obtenção da matriz H
He <- iris2 %>% 
  dplyr::filter(partition == "treino") %>%
  dplyr::select(Petal.Length, Petal.Width) %>%
  ks::Hpi()

# kernel density estimation com a H estimada
kdeHe <- iris2 %>% 
  dplyr::filter(partition == "treino") %>%
  dplyr::select(Petal.Length, Petal.Width) %>%
  ks::kde(., H=He)
```

\ 

```{r}
#| label: fig-kdeiris
#| fig-cap: Densidade estimada do conjunto de dados de treino iris para comprimento e largura da pétala.
#| fig-height: 3.5

pteste <- iris2 %>%
  dplyr::filter(partition == "teste") %>%
  dplyr::select(Petal.Length, Petal.Width) %>%
  as.matrix()

par(mar = c(3,4,4,4), pin = c(4, 2.75))

image(kdeHe$eval.points[[1]],kdeHe$eval.points[[2]],
      kdeHe$estimate, xlab = 'Petal Length',
      ylab = 'Petal Width')
points(kdeHe$x, col = alpha("black", 0.3))
points(pteste, col = "blue", pch = 8)
```


Como a classificação da espécia Setosa é novamente óbvia, apenas a classificação para as demais espécies será feita. As densidades para Virgínica e Versicolor são estimadas a seguir.

\ 

```{r}
#| label: kde-species

# H versicolor
He_versi <- iris2 %>% 
  dplyr::filter(partition == "treino", Species == "versicolor") %>%
  dplyr::select(Petal.Length, Petal.Width) %>%
  ks::Hpi()

# kde versi
kdeHe_versi <- iris2 %>% 
  dplyr::filter(partition == "treino", Species == "versicolor") %>%
  dplyr::select(Petal.Length, Petal.Width) %>%
  ks::kde(., H=He_versi)

# H virginica
He_virg <- iris2 %>% 
  dplyr::filter(partition == "treino", Species == "virginica") %>%
  dplyr::select(Petal.Length, Petal.Width) %>%
  ks::Hpi()

# kde virginica
kdeHe_virg <- iris2 %>% 
  dplyr::filter(partition == "treino", Species == "virginica") %>%
  dplyr::select(Petal.Length, Petal.Width) %>%
  ks::kde(., H=He_virg)
```

\ 

Os valores das densidades para os pontos de teste são dados na @tbl-classificakde a seguir, assim como as suas classificações.

\ 

```{r}
#| label: tbl-classificakde
#| tbl-cap: Classificação dos pontos de teste com densidades estimadas para Virgínica e Versicolor.

pteste2 <- iris2 %>% 
  dplyr::filter(partition == "teste", Species != "setosa") %>%
  dplyr::select(Petal.Length, Petal.Width) %>%
  as.matrix()

iris2 %>% 
  dplyr::filter(partition == "teste", Species != "setosa") %>%
  dplyr::select(Petal.Length, Petal.Width, Species) %>%
  mutate(dens.versi = predict(kdeHe_versi, x = pteste2),
         dens.virg = predict(kdeHe_virg, x = pteste2),
         `Classificação` = if_else(dens.versi > dens.virg, "versicolor", "virginica")) %>%
  knitr::kable(align = "cclccl",
               digits = 2)

```


\ 

## k-Vizinhos mais Próximos (KNN)

O método de k-vizinhos mais próximos ($k$ *nearest neighbors*) pretende classificar um determinado ponto $\mathbf{x}_0$ a partir dos $k$ pontos mais próximos. A classificação é dada em função da menor distância euclidiana entre $\mathbf{x}_0$ e os pontos de treinamento e, se houver empate, a escolha é feita aleatoriamente. Enquanto há muitas medidas de distância possíveis, a mais comum é a distância euclidiana, dada por

$$
d(\mathbf{x}_i, \mathbf{x}_0) = \sqrt{ \sum_{l=1}^p (x_{il} - x_{0l})^2 } = ||\mathbf{x}_i - \mathbf{x}_0||_2.
$$
\ 

A @fig-scatteriris2 apresenta um gráfico de dispersão do conjunto de dados em função das variáveis de interesse, no qual as cores indicam a espécie e as formas indicam a partição de treino ou teste. As unidades de teste foram selecionadas de forma arbitrária visando ilustrar as limitações desse método. 

Enquanto é esperado que o ponto de teste de espécie Setosa seja classificado corretamente, não se pode esperar uma classificação correta dos pontos de teste de espécie Versicolor e Virginica, visto que estão numa fronteira difusa entre os grupos.

\ 

```{r}
#| label: fig-scatteriris2
#| fig-cap: Gráfico de dispersão do conjunto de dados iris.
#| fig-height: 3

# gráfico
ggplot() +
  geom_point(data = iris2[iris2$partition == "teste",], aes(x = Petal.Length, y = Petal.Width, color = Species), shape = 8) +
  geom_point(data = iris2[iris2$partition == "treino",], aes(x = Petal.Length, y = Petal.Width, color = Species), shape = 16) +
  theme_bw() +
  labs(x = "Petal.Length",
    y = "Petal.Width",
    color = "Espécie",
    shape = "Partição")

```
\ 

A @tbl-knniris apresenta a espécie real dos pontos de teste, bem como a classificação utilizando os 3 e cinco vizinhos mais próximos (knn = 3 e knn = 5 respectivamente). O comportamento é exatamente o esperado.

\ 

```{r}
#| label: tbl-knniris
#| tbl-cap: Classificação dos pontos de teste do conjunto de dados iris.

treino <- iris2 %>% filter(partition == "treino")
teste <- iris2 %>% filter(partition == "teste")

knn3 <- knn(train = treino[, c(1,2)],
    test = teste[, c(1,2)],
    cl = treino$Species,
    k = 3)

knn5 <- knn(train = treino[, c(1,2)],
    test = teste[, c(1,2)],
    cl = treino$Species,
    k = 5)

data.frame(
  teste$Species,
  knn3,
  knn5
) %>%
  knitr::kable(
    col.names = c("Espécie real", "k = 3", "k = 5")
  )
```
\ 

Finalmente, projeta-se a classificação dos pontos de acordo com os dois algoritmos KNN utilizados na @fig-knniris. O conjunto completo é exibido no gráfico central da segunda linha.

\ 


```{r}
#| label: fig-knniris
#| fig-cap: Classificação dos pontos de teste do conjunto de dados iris.
#| fig-height: 5
#| fig-width: 6.5
#| echo: false

base <- ggplot(iris2, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point(alpha = 0.4, shape = 16) +
  theme_bw() +
  labs(x = "Treino",
    y = "",
    color = "Espécie")+
  theme(legend.position = "none")

knnteste <- teste %>%
  mutate(knn3 = knn3, knn5 = knn5)

plotknn3 <- base +
  geom_point(data = teste, aes(color = knn3), shape = 8, size = 2.75)+
  labs(x = "knn3", y = "")

plotknn5 <- base +
  geom_point(data = teste, aes(color = knn5), shape = 8, size = 2.75)+
  labs(x = "knn5", y = "")

plot_grid(
  plot_grid(plotknn3, plotknn5,nrow = 1, ncol = 2),
  plot_grid(NULL, base, NULL, nrow = 1, rel_widths = c(0.5, 1, 0.5)),
  nrow = 2)
```


\ 

# Questão 16

Estudar o pacote `ks` do R e apresentar um exemplo.

------------------------------------------------------------------------

\ 


\ 

# Questão 17

Apresentar um exemplo com classificador LDA e QDA.

------------------------------------------------------------------------

Como auxiliar, será definido um método `is_cov`, para garantir que as matrizes de variância-covariância (que serão geradas aleatoriamente) satisfazem as condições necessárias. Será definida também uma função que calcula a acurácia de um modelo.

```{r}
is_cov <- function(Sigma) {

  is_square <- nrow(Sigma) == ncol(Sigma)
  
  is_symmetric <- all(t(Sigma) == Sigma)
  
  positive_diag <- all(diag(Sigma) > 0)
  
  eigen_greater_zero <- all(with(eigen(Sigma), values) > 0)
  
  cov_smaller_than_sds <- TRUE

  for (row in 1:p) {
    for (col in row:p) {
      cov_smaller_than_sds <- cov_smaller_than_sds & (
        abs(Sigma[row, col]) <= sqrt(Sigma[row, row] * Sigma[col, col])
      )
    }
  }

  all(
    is_square, is_symmetric, positive_diag,
    cov_smaller_than_sds, eigen_greater_zero
  )
}

accuracy <- function(model, data, response) {
  prediction <- predict(model, data)$class 

  round(sum(prediction == response) / length(response), 3)
}
```

Em seguida, será criada uma matriz de variâncias-covariâncias, a partir da qual será criada uma normal multivariada. Essa matriz será multiplicada pela sua transposta, de modo que satisfaça as condições necessárias para ser uma matriz de variância-covariância.

\ 

```{r}
p <- 3

set.seed(exp(1))

Sigma <- rnorm(p ^ 2) %>%
    matrix(p, p) %>%
    (function(mat) t(mat) %*% mat)

round(Sigma, 3)
```

```{r}
is_cov(Sigma)
```

\ 

Criando os dados, com mesma variância e vetores de média diferentes (também gerados aleatoriamente), teremos:

\ 

```{r}
#| label: fig-linear1
#| fig-cap: Correlações e histogramas das variáveis explicativas.

muA <- rnorm(p, mean = -2)
muB <- rnorm(p, mean = 0)
nA <- 169
nB <- 196

linear <- rbind(
    MASS::mvrnorm(n = nA, mu = muA, Sigma = Sigma) %>%
      as_tibble() %>%
      cbind(Grupo = "A"),
    MASS::mvrnorm(n = nB, mu = muB, Sigma = Sigma) %>%
      as_tibble() %>%
      cbind(Grupo = "B")
)

pct80 <- as.integer(.8 * nrow(linear))

rows_train <- c(rep(TRUE, pct80), rep(FALSE, nrow(linear) - pct80)) %>%
  sample()

train_linear <- linear %>%
  filter(rows_train)

test_linear <- linear %>%
  filter(!rows_train)

chart.Correlation(train_linear %>% dplyr::select(V1:V3))
```

```{r}
#| label: fig-linear2
#| fig-cap: Gráficos de dispersão das variáveis explicativas com marcadores por classes linearmente separáveis.

with(train_linear, pairs(
  train_linear %>% dplyr::select(V1:V3),
  col = with(train_linear, c(A = "black", B = "pink")[Grupo]),
  upper.panel = function(...) {}
))
```

\ 

Como esperado, muitos dados estão espalhados e em regiões sobrepostas, com várias delas correlacionadas, o que dá indícios da possibilidade de rotações.

A biblioteca `MASS` possui um comando pré-implementado que ajusta um LDA.

A principal forma de se definir o modelo é:

-   `formula`: Sintaxe de fórmula análoga a qualquer outra modelagem em R.

-   `data`: Dados a partir dos quais os coeficientes da fórmula serão ajustados. O argumento é opcional, caso cada elemento da fórmula esteja individualmente definido.

Existe também uma forma alternativa não muito usual:

-   `x`: Conjunto de dados com variáveis explicativas;

-   `grouping`: Variável resposta contendo as categorias a serem previstas.

O método também permite selecionar as probabilidades de cada classe, métodos de estimação e se o modelo será treinado com validação cruzada ou não. Por padrão, ele não utiliza validação cruzada.

Ajustando três LDAs com apenas duas variáveis cada (e todos os argumentos default), obtemos:

\ 

```{r}
incomplete_ldas <- list(
  V1V2 = lda(Grupo ~ V1 + V2, train_linear),
  V2V3 = lda(Grupo ~ V2 + V3, train_linear),
  V1V3 = lda(Grupo ~ V1 + V3, train_linear)
)

with(incomplete_ldas, V1V2)
```

\ 

O resultado do método mostra a probabilidade de ocorrência de cada grupo (quando não definido manualmente, é utilizada a probabilidade nos dados), assim como a média e os coeficientes dos discriminantes. É possível plotar os resultados assim:

```{r}
#| include: false

# salva plots
png("plot1.png", width = 800, height = 600)
plot(incomplete_ldas[[1]], main = "Grupo ~ V1 + V2")

png("plot2.png", width = 800, height = 600)
plot(incomplete_ldas[[2]], main = "Grupo ~ V2 + V3")

png("plot3.png", width = 800, height = 600)
plot(incomplete_ldas[[3]], main = "Grupo ~ V1 + V3")

dev.off()
```

::: {#fig-elephants layout-ncol=2, layout-nrow=2}

![Grupo ~ V1 + V2](plot1.png){#fig-v1v2}

![Grupo ~ V2 + V3](plot2.png){#fig-v2v3}

![Grupo ~ V1 + V3](plot3.png){#fig-v1v3}

Histogramas dos LDAs ajustados com duas variáveis explicativas.
:::

Testando a acurácia com apenas duas variáveis explicativas, se obtém:

\ 

```{r}
for (mdl in names(incomplete_ldas)) paste(
  "Acurácia:", mdl, accuracy(
    incomplete_ldas[[mdl]],
    test_linear,
    with(test_linear, Grupo)), '\n'
) %>%
  cat()
```

```{r}
(lda_full <- lda(Grupo ~ V1 + V2 + V3, train_linear))

paste("Acurácia:", accuracy(lda_full, test_linear, with(test_linear, Grupo))) %>%
  cat()
```

\ 

Com apenas duas variáveis, nenhum dos classificadores atinge acurácia maior que 70%. Utilizando as três, o resultado ultrapassa 90% de acurácia.

Ajustando um modelo QDA para os mesmos dados (que não é o uso mais adequado da técnica, uma vez que as variâncias são iguais e eles são linearmente separáveis), o resultado é:

\ 

```{r}
paste("Acurácia:", accuracy(
  qda(Grupo ~ V1 + V2 + V3, train_linear),
  test_linear, with(test_linear, Grupo))
) %>%
  cat()
```

\ 

Que é um resultado inferior ao modelo LDA completo.

Para uma implementação do QDA, os grupos serão modificados a partir dos mesmos dados, forçando que a separação não-linear seja mais adequada que uma separação linear. Os resultados são expostos a seguir.

\ 

```{r}
#| label: fig-nonlinear
#| fig-cap: Gráficos de dispersão das variáveis explicativas com marcadores por classes não-linearmente separáveis.

nonlinear <- linear %>%
  mutate(
    Grupo = ifelse(
      (
        abs(V1 ^ 2 * rgamma(n(), 3) * V3 - V2 ^ 3) >
          mean(-V1 + V2 - V3) / 2 + rnorm(n(), 10)
      ),
      "A", "B"
    )
  )

train_nonlinear <- nonlinear %>%
  filter(rows_train)

test_nonlinear <- nonlinear %>%
  filter(!rows_train)

with(train_nonlinear, pairs(
  train_nonlinear %>% dplyr::select(V1:V3),
  col = with(train_nonlinear, c(A = "black", B = "pink")[Grupo]),
  upper.panel = function(...) {}
))
```

\ 

Se observa, em todas as variáveis, regiões curvadas onde seria possível estabelecer limites de classificação. Ainda há algum nível de confusão entre as regiões, de forma que a separçaão não é perfeita.

Ajustando um QDA com as variáveis nessa estrutura, obtemos:

\ 

```{r}
incomplete_qdas <- list(
  V1V2 = qda(Grupo ~ V1 + V2, train_linear),
  V2V3 = qda(Grupo ~ V2 + V3, train_linear),
  V1V3 = qda(Grupo ~ V1 + V3, train_linear)
)

for (mdl in names(incomplete_qdas)) paste(
  "Acurácia:", mdl, accuracy(
    incomplete_qdas[[mdl]],
    test_nonlinear,
    with(test_nonlinear, Grupo)), '\n'
) %>%
  cat()
```

```{r}
paste(
  "Acurácia:", accuracy(
    qda(Grupo ~ V1 + V2 + V3, train_nonlinear),
    test_nonlinear,
    with(test_nonlinear, Grupo)), '\n'
) %>%
  cat()
```

\ 

Para estes dados, uma das classificações com duas variáveis (V1 com V2) já chegou a uma acurácia próxima de 80%. Utilizando as três, mais de 93% dos pontos são classificados corretamente.

Ajustando um LDA para os dados não-lineares, a título de comparação, se obtém:

\ 

```{r}
accuracy(
  lda(Grupo ~ V1 + V2 + V3, train_nonlinear),
  test_nonlinear, with(test_nonlinear, Grupo)
)
```

\ 

Que é uma precisão mais baixa que a precisão do QDA, como esperado pelo fato de a aplicação LDA não ser o uso mais adequado do método para esses dados.

Apesar disso, mesmo não sendo o uso mais adequado, o ajuste LDA ainda acertou mais de 75% das classificações.

<!-- {{< pagebreak >}} -->

<!-- # Referências -->
